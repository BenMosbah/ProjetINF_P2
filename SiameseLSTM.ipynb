{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "## edit the link to be shorter and accessible from root folder\n",
    "train = pd.read_csv(\"data/train.csv\",header=None,names=['PairNb','Q1id','Q2id','Q1RT','Q2RT','label'])\n",
    "test = pd.read_csv(\"data/test.csv\",header=None,names=['PairNb','Q1id','Q2id','Q1RT','Q2RT','label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remarque : Les données fournies ont l'air complètes et il y a pas l'air d'y avoir des fautes d'orthographe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing a second method : Siamese LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Getting the raw features that we will be working on :\n",
    "train_X, train_y = train.iloc[:,:-1], train.iloc[:,-1]\n",
    "test_X = test.iloc[:,:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Text cleaning method. Proposée par Elior Cohen (il faudrait voir ce que c'est)\n",
    "import re\n",
    "def text2word(text):\n",
    "    text = str(text).lower()\n",
    "\n",
    "    # Text cleaning\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"e-mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = text.split()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Loading the embedding file (we use GoogleNews)\n",
    "embedding_file = \"GoogleNews-vectors-negative300.bin.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from gensim.models import KeyedVectors\n",
    "# Prepare vocabulary and look up list\n",
    "vocabulary = dict()\n",
    "inverse_vocabulary = ['<unk>']  # placeholder for the [0, 0, ....0] embedding / padding\n",
    "word2vec = KeyedVectors.load_word2vec_format(embedding_file, binary=True)\n",
    "stops = set(stopwords.words(\"english\"))\n",
    "\n",
    "questions_cols = ['Q1RT', 'Q2RT']\n",
    "\n",
    "# Iterate over the questions only of the training dataset\n",
    "for dataset in [train_X, test_X]:\n",
    "    for index, row in dataset.iterrows():\n",
    "        # Iterate through the text of both questions of the row\n",
    "        for question in questions_cols:\n",
    "            q2n = []  # q2n -> question to numbers representation\n",
    "            for word in text2word(row[question]):\n",
    "                # Check for unwanted words\n",
    "                if word in stops and word not in word2vec.vocab:\n",
    "                    continue\n",
    "                # add new word into vocabulary and assign look up tables with both word & number\n",
    "                if word not in vocabulary:\n",
    "                    vocabulary[word] = len(inverse_vocabulary)\n",
    "                    q2n.append(len(inverse_vocabulary))\n",
    "                    inverse_vocabulary.append(word)\n",
    "                else:\n",
    "                    q2n.append(vocabulary[word])\n",
    "\n",
    "            # Replace questions with number representations from vocabulary look-up table\n",
    "            dataset.set_value(index, question, q2n)\n",
    "\n",
    "embedding_dim = 300\n",
    "# This will be the embedding matrix, generated from gaussian distribution for words that do not appear at all. (default values)\n",
    "embeddings = 1 * np.random.randn(len(vocabulary) + 1, embedding_dim)  \n",
    "embeddings[0] = 0  # So that the padding will be ignored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PairNb</th>\n",
       "      <th>Q1id</th>\n",
       "      <th>Q2id</th>\n",
       "      <th>Q1RT</th>\n",
       "      <th>Q2RT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>199954</td>\n",
       "      <td>384085</td>\n",
       "      <td>[1, 2, 3, 4, 3, 5, 6]</td>\n",
       "      <td>[1, 2, 4, 3, 7, 6, 8, 9, 10, 2, 11, 12]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>128681</td>\n",
       "      <td>237407</td>\n",
       "      <td>[1, 2, 3, 13, 14, 15, 16, 17, 18]</td>\n",
       "      <td>[1, 2, 4, 19, 13, 20, 21, 22, 18, 23]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>170846</td>\n",
       "      <td>240621</td>\n",
       "      <td>[24, 3, 25, 26, 27, 28, 29, 30, 31, 3, 32, 33,...</td>\n",
       "      <td>[36, 21, 3, 37, 26, 29, 38, 3, 32, 33]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>55110</td>\n",
       "      <td>177468</td>\n",
       "      <td>[39, 40, 41, 24, 42, 43]</td>\n",
       "      <td>[10, 39, 40, 24, 42, 43]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>425513</td>\n",
       "      <td>400256</td>\n",
       "      <td>[1, 44, 45, 14, 16, 46, 47, 16, 48, 49]</td>\n",
       "      <td>[1, 50, 51, 52, 35, 53, 54, 48, 55, 56]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PairNb    Q1id    Q2id                                               Q1RT  \\\n",
       "0       0  199954  384085                              [1, 2, 3, 4, 3, 5, 6]   \n",
       "1       1  128681  237407                  [1, 2, 3, 13, 14, 15, 16, 17, 18]   \n",
       "2       2  170846  240621  [24, 3, 25, 26, 27, 28, 29, 30, 31, 3, 32, 33,...   \n",
       "3       3   55110  177468                           [39, 40, 41, 24, 42, 43]   \n",
       "4       4  425513  400256            [1, 44, 45, 14, 16, 46, 47, 16, 48, 49]   \n",
       "\n",
       "                                      Q2RT  \n",
       "0  [1, 2, 4, 3, 7, 6, 8, 9, 10, 2, 11, 12]  \n",
       "1    [1, 2, 4, 19, 13, 20, 21, 22, 18, 23]  \n",
       "2   [36, 21, 3, 37, 26, 29, 38, 3, 32, 33]  \n",
       "3                 [10, 39, 40, 24, 42, 43]  \n",
       "4  [1, 50, 51, 52, 35, 53, 54, 48, 55, 56]  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We check that Q1RT and Q2RT are now lists of numbers indicating which words from the vocabulary we built are being used in\n",
    "#the question\n",
    "train_X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build the embedding matrix\n",
    "for word, index in vocabulary.items():\n",
    "    if word in word2vec.vocab:\n",
    "        # adds in the embedding dimension's values per row from Google's pre-trained model\n",
    "        embeddings[index] = word2vec.word_vec(word)\n",
    "\n",
    "del word2vec # to not clog up memory on the system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To obtain the max length of the longest question in train/test datasets\n",
    "max_seq_length = max(train_X.Q1RT.map(lambda x: len(x)).max(),\n",
    "                     train_X.Q2RT.map(lambda x: len(x)).max(),\n",
    "                     test_X.Q1RT.map(lambda x: len(x)).max(),\n",
    "                     test_X.Q2RT.map(lambda x: len(x)).max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools as itertools\n",
    "import sklearn as skl\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "#Preprocessing data to be fed into the siamese LSTM\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "LSTM_train_X, LSTM_valid_X, LSTM_train_y, LSTM_valid_y = train_test_split(train_X, train_y, test_size=0.2)\n",
    "\n",
    "# Split to dictionaries for training, validation and test data\n",
    "LSTM_train_X = {'left': LSTM_train_X.Q1RT, 'right': LSTM_train_X.Q2RT}\n",
    "LSTM_valid_X = {'left': LSTM_valid_X.Q1RT, 'right': LSTM_valid_X.Q2RT}\n",
    "LSTM_test_X = {'left': test_X.Q1RT, 'right': test_X.Q2RT}\n",
    "\n",
    "# Convert y-values (labels) to their numpy representations\n",
    "LSTM_train_y = LSTM_train_y.values\n",
    "LSTM_valid_y = LSTM_valid_y.values\n",
    "\n",
    "# Zero padding\n",
    "for dataset, side in itertools.product([LSTM_train_X, LSTM_valid_X], ['left', 'right']):\n",
    "    dataset[side] = pad_sequences(dataset[side], maxlen=max_seq_length)\n",
    "\n",
    "# Checking consistent shapes for data using assert\n",
    "assert LSTM_train_X['left'].shape == LSTM_train_X['right'].shape # left and right columns are of the same length\n",
    "assert len(LSTM_train_X['left']) == len(LSTM_train_y) # check if number of rows tally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Defining the loss and the function used in the last layer of Siamese LSTM\n",
    "def weighted_log_loss(y_true, y_pred):\n",
    "    a = 0.165/0.37\n",
    "    b = (1-0.165)/(1-0.37)\n",
    "    score = a*y_true*K.log(y_pred+0.00001) + b*(1.0 - y_true)*K.log(1.0 - y_pred+0.00001)\n",
    "    return -K.mean(score)\n",
    "\n",
    "def exponent_neg_manhattan_distance(left, right):\n",
    "    return K.exp(-K.sum(K.abs(left-right), axis=1, keepdims=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 67)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 67)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 67, 300)      5807400     input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 50)           70200       embedding_1[0][0]                \n",
      "                                                                 embedding_1[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 1)            0           lstm_1[0][0]                     \n",
      "                                                                 lstm_1[1][0]                     \n",
      "==================================================================================================\n",
      "Total params: 5,877,600\n",
      "Trainable params: 70,200\n",
      "Non-trainable params: 5,807,400\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Train on 64080 samples, validate on 16020 samples\n",
      "Epoch 1/15\n",
      " 6144/64080 [=>............................] - ETA: 2:32 - loss: 0.6255 - weighted_log_loss: 0.6255 - acc: 0.4382"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-dec2a27e3291>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;31m# Start training\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m model_trained = maLSTM.fit([LSTM_train_X['left'], LSTM_train_X['right']], LSTM_train_y, batch_size=batch_size, epochs=epochs,\n\u001b[1;32m---> 51\u001b[1;33m                             validation_data=([LSTM_valid_X['left'], LSTM_valid_X['right']], LSTM_valid_y), verbose=1)\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1439\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1440\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Embedding, LSTM, concatenate, Dense, Dropout, Lambda\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "import keras.backend as K\n",
    "\n",
    "# Model hyperparameters\n",
    "hidden_layer_nodes = 50\n",
    "batch_size = 128\n",
    "epochs = 15\n",
    "\n",
    "# declare left and right inputs\n",
    "left_input = Input(shape=(max_seq_length,), dtype='int32')\n",
    "right_input = Input(shape=(max_seq_length,), dtype='int32')\n",
    "\n",
    "# Create an embedding layer to convert words to their embeddings\n",
    "embedding_layer = Embedding(len(embeddings), embedding_dim, weights=[embeddings], input_length=max_seq_length, trainable=False)\n",
    "\n",
    "# Convert inputs into word embeddings\n",
    "encoded_left = embedding_layer(left_input)\n",
    "encoded_right = embedding_layer(right_input)\n",
    "\n",
    "# lstm layer that will return an output the size of the number of hidden layer nodes\n",
    "shared_lstm = LSTM(hidden_layer_nodes)\n",
    "\n",
    "# run both inputs through shared lstm\n",
    "output_left = shared_lstm(encoded_left)\n",
    "output_right = shared_lstm(encoded_right)\n",
    "\n",
    "\n",
    "def custom_merge(x):\n",
    "    return exponent_neg_manhattan_distance(x[0],x[1])\n",
    "\n",
    "merged_vector = Lambda(custom_merge,output_shape=lambda x: (x[0][0], 1))([output_left, output_right])    \n",
    "\n",
    "# finish off model with output layer\n",
    "maLSTM = Model([left_input, right_input], [merged_vector])\n",
    "\n",
    "# optimizer\n",
    "optimizer = Adam(clipnorm=.9)\n",
    "\n",
    "# complete model\n",
    "maLSTM.compile(optimizer=optimizer, loss=weighted_log_loss, metrics=[weighted_log_loss,'accuracy'])\n",
    "\n",
    "# summarize the model\n",
    "print(maLSTM.summary())\n",
    "\n",
    "# Start training\n",
    "model_trained = maLSTM.fit([LSTM_train_X['left'], LSTM_train_X['right']], LSTM_train_y, batch_size=batch_size, epochs=epochs,\n",
    "                            validation_data=([LSTM_valid_X['left'], LSTM_valid_X['right']], LSTM_valid_y), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# summarize history for accuracy\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(model_trained.history['acc'])\n",
    "plt.plot(model_trained.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'valid'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# summarize history for loss\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(model_trained.history['loss'])\n",
    "plt.plot(model_trained.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'valid'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LSTM_test_X['left'] = pad_sequences(LSTM_test_X['left'], maxlen=max_seq_length)\n",
    "LSTM_test_X['right'] = pad_sequences(LSTM_test_X['right'], maxlen=max_seq_length)\n",
    "prob_y = maLSTM.predict([LSTM_test_X['left'], LSTM_test_X['right']])\n",
    "\n",
    "## dump into .csv for Kaggle submission\n",
    "submission = pd.DataFrame()\n",
    "submission['Id'] = test_X['PairNb']\n",
    "submission['Score'] = prob_y\n",
    "submission.to_csv(\"submission_siameseLSTM.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
