{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='red'>Quora Question Pairs - Combined LSTM Model</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'Levenshtein'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-4003a2450e69>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mLevenshtein\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mleven\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mKeyedVectors\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmath\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msqrt\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'Levenshtein'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import itertools as itertools\n",
    "import sklearn as skl\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk as nk\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, make_scorer, confusion_matrix\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Embedding, LSTM, Merge, Dense, Dropout, concatenate\n",
    "from keras.optimizers import Adadelta\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import keras.backend as K\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import re\n",
    "import Levenshtein as leven\n",
    "from gensim.models import KeyedVectors\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import datasets and clean data, for practice.\n",
    "The training dataset provided will be split into train-test to validate model's accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## edit the link to be shorter and accessible from root folder\n",
    "train = pd.read_csv(r'C:\\Users\\lim_j\\Google Drive\\Technical Skills\\Kaggle\\Quora Question Pairs\\train.csv')\n",
    "test = pd.read_csv(r'C:\\Users\\lim_j\\Google Drive\\Technical Skills\\Kaggle\\Quora Question Pairs\\test.csv')\n",
    "embedding_file = r'C:\\Users\\lim_j\\Google Drive\\Technical Skills\\Kaggle\\Quora Question Pairs\\GoogleNews-vectors-negative300.bin.gz'\n",
    "model_dir = r'C:\\Users\\lim_j\\Google Drive\\Technical Skills\\Kaggle\\Quora Question Pairs\\Model'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>404290.000000</td>\n",
       "      <td>404290.000000</td>\n",
       "      <td>404290.000000</td>\n",
       "      <td>404290.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>202144.500000</td>\n",
       "      <td>217243.942418</td>\n",
       "      <td>220955.655337</td>\n",
       "      <td>0.369198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>116708.614502</td>\n",
       "      <td>157751.700002</td>\n",
       "      <td>159903.182629</td>\n",
       "      <td>0.482588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>101072.250000</td>\n",
       "      <td>74437.500000</td>\n",
       "      <td>74727.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>202144.500000</td>\n",
       "      <td>192182.000000</td>\n",
       "      <td>197052.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>303216.750000</td>\n",
       "      <td>346573.500000</td>\n",
       "      <td>354692.500000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>404289.000000</td>\n",
       "      <td>537932.000000</td>\n",
       "      <td>537933.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  id           qid1           qid2   is_duplicate\n",
       "count  404290.000000  404290.000000  404290.000000  404290.000000\n",
       "mean   202144.500000  217243.942418  220955.655337       0.369198\n",
       "std    116708.614502  157751.700002  159903.182629       0.482588\n",
       "min         0.000000       1.000000       2.000000       0.000000\n",
       "25%    101072.250000   74437.500000   74727.000000       0.000000\n",
       "50%    202144.500000  192182.000000  197052.000000       0.000000\n",
       "75%    303216.750000  346573.500000  354692.500000       1.000000\n",
       "max    404289.000000  537932.000000  537933.000000       1.000000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 404290 entries, 0 to 404289\n",
      "Data columns (total 6 columns):\n",
      "id              404290 non-null int64\n",
      "qid1            404290 non-null int64\n",
      "qid2            404290 non-null int64\n",
      "question1       404290 non-null object\n",
      "question2       404288 non-null object\n",
      "is_duplicate    404290 non-null int64\n",
      "dtypes: int64(4), object(2)\n",
      "memory usage: 18.5+ MB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### There are 2 null values under column question2, hence we will have to remove these 2 pairs of questions with null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>105780</th>\n",
       "      <td>105780</td>\n",
       "      <td>174363</td>\n",
       "      <td>174364</td>\n",
       "      <td>How can I develop android app?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201841</th>\n",
       "      <td>201841</td>\n",
       "      <td>303951</td>\n",
       "      <td>174364</td>\n",
       "      <td>How can I create an Android app?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id    qid1    qid2                         question1 question2  \\\n",
       "105780  105780  174363  174364    How can I develop android app?       NaN   \n",
       "201841  201841  303951  174364  How can I create an Android app?       NaN   \n",
       "\n",
       "        is_duplicate  \n",
       "105780             0  \n",
       "201841             0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[train.isnull().any(axis=1)]\n",
    "train = train.dropna(axis=0, how='any')\n",
    "\n",
    "# Verify that rows with null values have been removed\n",
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count number of words in a question\n",
    "def words(question):\n",
    "    return len(question.split())\n",
    "\n",
    "# Average length of a word in a question\n",
    "def avg_word_length(question):\n",
    "    total_words_len = 0\n",
    "    no_of_words = 0\n",
    "    question = question.split()\n",
    "    for word in question:\n",
    "        total_words_len += len(word)\n",
    "        no_of_words += 1\n",
    "    return total_words_len/no_of_words\n",
    "\n",
    "# Number of characters in a question\n",
    "def char_count(question):\n",
    "    return len(question)\n",
    "\n",
    "# Caps count of question, only takes into account whether the first character of each word is in uppercase\n",
    "def caps_count(question):\n",
    "    question = question.split()\n",
    "    count = 0\n",
    "    for word in question:\n",
    "        if word[0].isupper():\n",
    "            count += 1\n",
    "    \n",
    "    return count\n",
    "\n",
    "# Jaccard Similiarity Coefficient\n",
    "# Obtain the Jaccard Similiarity Coeefficient between 2 questions\n",
    "# (X intersect Y) / (X union Y)\n",
    "def jaccard_coeff(dataframe):\n",
    "    question1 = dataframe['question1']\n",
    "    question2 = dataframe['question2']\n",
    "    question1 = question1.split(' ')\n",
    "    question2 = question2.split(' ')\n",
    "    shared_words = 0\n",
    "    total_words = len(question1) + len(question2)\n",
    "    \n",
    "    for word1 in question1:\n",
    "        for word2 in question2:\n",
    "            if word1 == word2:\n",
    "                shared_words += 1\n",
    "    \n",
    "    if (total_words-shared_words) == 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return shared_words/(total_words-shared_words)\n",
    "    \n",
    "# Levenshtein distance\n",
    "# Obtain the Levenshtein distance between 2 questions\n",
    "def levenshtein(dataframe):\n",
    "    return leven.distance(dataframe['question1'], dataframe['question2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Slicing imported dataframe into question1 series, question2 series and questions dataframe\n",
    "q1 = train.iloc[:,3]\n",
    "q2 = train.iloc[:,4]\n",
    "q = train.iloc[:,3:5]\n",
    "dup = train.iloc[:,5]\n",
    "\n",
    "# Creating new features using feature engineering functions\n",
    "word_len_diff = abs(q1.apply(words) - q2.apply(words))\n",
    "avg_word_len_diff = abs(q1.apply(avg_word_length) - q2.apply(avg_word_length))\n",
    "char_diff = abs(q1.apply(char_count) - q2.apply(char_count))\n",
    "caps_diff = abs(q1.apply(caps_count) - q2.apply(caps_count))\n",
    "jaccard = q.apply(jaccard_coeff, axis=1)\n",
    "leven_dist = q.apply(levenshtein, axis=1)\n",
    "\n",
    "# Creating a new dataframe with values of new features\n",
    "classic_feat = pd.DataFrame({'word_len_diff': word_len_diff, 'avg_word_len_diff': avg_word_len_diff, \n",
    "                             'char_diff': char_diff, 'caps_diff': caps_diff, 'jaccard': jaccard, \n",
    "                             'leven_dist': leven_dist, 'duplicate': dup})\n",
    "classic_feat = classic_feat[['word_len_diff', 'avg_word_len_diff', 'char_diff', 'caps_diff', 'jaccard', 'leven_dist', 'duplicate']]\n",
    "\n",
    "# Create train = true/false boolean column for train-test split\n",
    "classic_feat['is_train'] = np.random.uniform(0, 1, len(classic_feat)) <= .75\n",
    "\n",
    "# Train-test dataframes split\n",
    "train, test = classic_feat[classic_feat['is_train']==True], classic_feat[classic_feat['is_train']==False]\n",
    "\n",
    "# Number of examplples for training and test dataframes\n",
    "print('# of examples in the training data:', len(train))\n",
    "print('# of examples in the test data:',len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 404288 entries, 0 to 404289\n",
      "Data columns (total 6 columns):\n",
      "id              404288 non-null int64\n",
      "qid1            404288 non-null int64\n",
      "qid2            404288 non-null int64\n",
      "question1       404288 non-null object\n",
      "question2       404288 non-null object\n",
      "is_duplicate    404288 non-null int64\n",
      "dtypes: int64(4), object(2)\n",
      "memory usage: 21.6+ MB\n"
     ]
    }
   ],
   "source": [
    "# Obtaining index of feature columns\n",
    "features = classic_feat.columns[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtaining y from the training data\n",
    "y = train['duplicate']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Munging\n",
    "Derived from Elior Cohen's data cleaning process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def text2word(text):\n",
    "    text = str(text).lower()\n",
    "\n",
    "    # Text cleaning\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"e-mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = text.split()\n",
    "    return text\n",
    "\n",
    "\n",
    "# Prepare embedding\n",
    "vocabulary = dict()\n",
    "inverse_vocabulary = ['<unk>']  # placeholder for the [0, 0, ....0] embedding / padding\n",
    "word2vec = KeyedVectors.load_word2vec_format(embedding_file, binary=True)\n",
    "stops = set(stopwords.words(\"english\"))\n",
    "\n",
    "questions_cols = ['question1', 'question2']\n",
    "\n",
    "# Iterate over the questions only of both training and test datasets\n",
    "for dataset in [train, test]:\n",
    "    for index, row in dataset.iterrows():\n",
    "\n",
    "        # Iterate through the text of both questions of the row\n",
    "        for question in questions_cols:\n",
    "            q2n = []  # q2n -> question numbers representation\n",
    "            \n",
    "            for word in text2word(row[question]):\n",
    "\n",
    "                # Check for unwanted words\n",
    "                if word in stops and word not in word2vec.vocab:\n",
    "                    continue\n",
    "\n",
    "                if word not in vocabulary:\n",
    "                    vocabulary[word] = len(inverse_vocabulary)\n",
    "                    q2n.append(len(inverse_vocabulary))\n",
    "                    inverse_vocabulary.append(word)\n",
    "                    \n",
    "                else:\n",
    "                    q2n.append(vocabulary[word])\n",
    "\n",
    "            # Replace questions as word to question as number representation\n",
    "            dataset.set_value(index, question, q2n)\n",
    "            \n",
    "embedding_dim = 300\n",
    "embeddings = 1 * np.random.randn(len(vocabulary) + 1, embedding_dim)  # This will be the embedding matrix\n",
    "embeddings[0] = 0  # So that the padding will be ignored\n",
    "\n",
    "# Build the embedding matrix\n",
    "for word, index in vocabulary.items():\n",
    "    if word in word2vec.vocab:\n",
    "        embeddings[index] = word2vec.word_vec(word)\n",
    "\n",
    "del word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-Validation data split for x and y values and zero padding of values to ensure consistency in shape of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# To obtain the max length of the longest question in train/test datasets\n",
    "max_seq_length = max(train.question1.map(lambda x: len(x)).max(),\n",
    "                     train.question2.map(lambda x: len(x)).max(),\n",
    "                     test.question1.map(lambda x: len(x)).max(),\n",
    "                     test.question2.map(lambda x: len(x)).max())\n",
    "\n",
    "# Split to train validation (80-20 split)\n",
    "validation_size = 0.2\n",
    "training_size = len(train)*(1-validation_size)\n",
    "\n",
    "# Breaking dataframe values into x (question strings) and y (is_duplicate = 1/0)\n",
    "x = train[questions_cols]\n",
    "y = train['is_duplicate']\n",
    "\n",
    "# Split data using train_test_split\n",
    "x_train, x_validation, y_train, y_validation = train_test_split(x, y, test_size=validation_size)\n",
    "\n",
    "# Split to dictionaries for training, validation and test data\n",
    "x_train = {'left': x_train.question1, 'right': x_train.question2}\n",
    "x_validation = {'left': x_validation.question1, 'right': x_validation.question2}\n",
    "x_test = {'left': test.question1, 'right': test.question2}\n",
    "\n",
    "# Convert y-values (labels) to their numpy representations\n",
    "y_train = y_train.values\n",
    "y_validation = y_validation.values\n",
    "\n",
    "# Zero padding\n",
    "for dataset, side in itertools.product([x_train, x_validation], ['left', 'right']):\n",
    "    dataset[side] = pad_sequences(dataset[side], maxlen=max_seq_length)\n",
    "\n",
    "# Checking consistent shapes for data using assert\n",
    "assert x_train['left'].shape == x_train['right'].shape\n",
    "assert len(x_train['left']) == len(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model building and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_1 (InputLayer)             (None, 213)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_2 (InputLayer)             (None, 213)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)          (None, 213, 300)      36427500    input_1[0][0]                    \n",
      "                                                                   input_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                    (None, 50)            70200       embedding_1[0][0]                \n",
      "                                                                   embedding_1[1][0]                \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)      (None, 100)           0           lstm_1[0][0]                     \n",
      "                                                                   lstm_1[1][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 1)             101         concatenate_1[0][0]              \n",
      "====================================================================================================\n",
      "Total params: 36,497,801\n",
      "Trainable params: 70,301\n",
      "Non-trainable params: 36,427,500\n",
      "____________________________________________________________________________________________________\n",
      "None\n",
      "Train on 323430 samples, validate on 80858 samples\n",
      "Epoch 1/1\n",
      "323430/323430 [==============================] - 3173s - loss: 0.1867 - acc: 0.7202 - val_loss: 0.1763 - val_acc: 0.7409\n"
     ]
    }
   ],
   "source": [
    "# Model hyperparameters\n",
    "hidden_layer_nodes = 50\n",
    "batch_size = 64\n",
    "epochs = 1\n",
    "\n",
    "# declare left and right inputs\n",
    "left_input = Input(shape=(max_seq_length, ), dtype='int32')\n",
    "right_input = Input(shape=(max_seq_length, ), dtype='int32')\n",
    "\n",
    "# Create an embedding layer to convert words to their embeddings\n",
    "embedding_layer = Embedding(len(embeddings), embedding_dim, weights=[embeddings], input_length=max_seq_length, trainable=False)\n",
    "\n",
    "# Convert inputs into word embeddings\n",
    "embedded_left = embedding_layer(left_input)\n",
    "embedded_right = embedding_layer(right_input)\n",
    "\n",
    "# lstm layer that will return an output the size of the number of hidden layer nodes\n",
    "shared_lstm = LSTM(hidden_layer_nodes)\n",
    "\n",
    "# run both inputs through shared lstm\n",
    "encoded_left = shared_lstm(embedded_left)\n",
    "encoded_right = shared_lstm(embedded_right)\n",
    "\n",
    "# concatenate results of both encoded vectors\n",
    "merged_vector = concatenate([encoded_left, encoded_right], axis=-1)\n",
    "\n",
    "# finish off model with output layer\n",
    "prediction = Dense(1, activation='relu')(merged_vector)\n",
    "\n",
    "# Define model hyperparameters such as optimiser and loss function\n",
    "model = Model(inputs=[left_input, right_input], outputs=[prediction])\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['accuracy'])\n",
    "\n",
    "# summarize the model\n",
    "print(model.summary())\n",
    "\n",
    "# Start training\n",
    "model_trained = model.fit([x_train['left'], x_train['right']], y_train, batch_size=batch_size, epochs=epochs,\n",
    "                            validation_data=([x_validation['left'], x_validation['right']], y_validation), verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate model scalar test loss and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80858/80858 [==============================] - 209s   \n",
      "Scalar test loss: 0.176279500335\n",
      "Model accuracy: 0.740928541398\n"
     ]
    }
   ],
   "source": [
    "test_loss_accuracy = model.evaluate([x_validation['left'], x_validation['right']], y_validation)\n",
    "print('Scalar test loss: ' + str(test_loss_accuracy[0]) + '\\nModel accuracy: ' + str(test_loss_accuracy[1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
