{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='red'>Quora Question Pairs</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\azhen\\Anaconda3\\envs\\ds\\lib\\site-packages\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import itertools as itertools\n",
    "import sklearn as skl\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, KFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, make_scorer, confusion_matrix\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Embedding, LSTM, Merge, Dense, Dropout, concatenate\n",
    "from keras.optimizers import Adadelta\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import keras.backend as K\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import re\n",
    "import Levenshtein as leven\n",
    "from gensim.models import KeyedVectors\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for reproducability\n",
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import datasets and clean data, for practice.\n",
    "The training dataset provided will be split into train-test to validate model's accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## edit the link to be shorter and accessible from root folder\n",
    "train = pd.read_csv(\"./train.csv\")\n",
    "test = pd.read_csv(\"./test.csv\")\n",
    "embedding_file = \"./GoogleNews-vectors-negative300.bin.gz\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1b85e6c12e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicates in the training dataset: 149263\n",
      "Number of non-duplicates in the training dataset: 255027\n",
      "Percentage of duplicates in training dataset: 36.92%\n",
      "Percentage of non-duplicates in training dataset: 63.08%\n"
     ]
    }
   ],
   "source": [
    "count_dups = np.sum(train.iloc[:,-1] == 1)\n",
    "# positive classes' percentage in the training dataset\n",
    "positive_dups = round(count_dups/train.shape[0]*100, 2)\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "plt.bar([\"duplicates\", \"non-duplicates\"], [count_dups, train.shape[0] - count_dups])\n",
    "plt.title('Number of Occurences')\n",
    "plt.yticks()\n",
    "plt.show()\n",
    "\n",
    "print(\"Number of duplicates in the training dataset: {}\".format(count_dups))\n",
    "print(\"Number of non-duplicates in the training dataset: {}\".format(train.shape[0] - count_dups))\n",
    "print(\"Percentage of duplicates in training dataset: {}%\".format(positive_dups))\n",
    "print(\"Percentage of non-duplicates in training dataset: {}%\".format(100.0 - positive_dups))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>404290.000000</td>\n",
       "      <td>404290.000000</td>\n",
       "      <td>404290.000000</td>\n",
       "      <td>404290.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>202144.500000</td>\n",
       "      <td>217243.942418</td>\n",
       "      <td>220955.655337</td>\n",
       "      <td>0.369198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>116708.614503</td>\n",
       "      <td>157751.700002</td>\n",
       "      <td>159903.182629</td>\n",
       "      <td>0.482588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>101072.250000</td>\n",
       "      <td>74437.500000</td>\n",
       "      <td>74727.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>202144.500000</td>\n",
       "      <td>192182.000000</td>\n",
       "      <td>197052.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>303216.750000</td>\n",
       "      <td>346573.500000</td>\n",
       "      <td>354692.500000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>404289.000000</td>\n",
       "      <td>537932.000000</td>\n",
       "      <td>537933.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  id           qid1           qid2   is_duplicate\n",
       "count  404290.000000  404290.000000  404290.000000  404290.000000\n",
       "mean   202144.500000  217243.942418  220955.655337       0.369198\n",
       "std    116708.614503  157751.700002  159903.182629       0.482588\n",
       "min         0.000000       1.000000       2.000000       0.000000\n",
       "25%    101072.250000   74437.500000   74727.000000       0.000000\n",
       "50%    202144.500000  192182.000000  197052.000000       0.000000\n",
       "75%    303216.750000  346573.500000  354692.500000       1.000000\n",
       "max    404289.000000  537932.000000  537933.000000       1.000000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## summary statistics\n",
    "train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 404290 entries, 0 to 404289\n",
      "Data columns (total 6 columns):\n",
      "id              404290 non-null int64\n",
      "qid1            404290 non-null int64\n",
      "qid2            404290 non-null int64\n",
      "question1       404289 non-null object\n",
      "question2       404288 non-null object\n",
      "is_duplicate    404290 non-null int64\n",
      "dtypes: int64(4), object(2)\n",
      "memory usage: 18.5+ MB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### There are 2 null values under feature 'question2' and 1 under feature 'question1'. Hence, we remove these 3 pairs of questions with null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>105780</th>\n",
       "      <td>105780</td>\n",
       "      <td>174363</td>\n",
       "      <td>174364</td>\n",
       "      <td>How can I develop android app?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201841</th>\n",
       "      <td>201841</td>\n",
       "      <td>303951</td>\n",
       "      <td>174364</td>\n",
       "      <td>How can I create an Android app?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363362</th>\n",
       "      <td>363362</td>\n",
       "      <td>493340</td>\n",
       "      <td>493341</td>\n",
       "      <td>NaN</td>\n",
       "      <td>My Chinese name is Haichao Yu. What English na...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id    qid1    qid2                         question1  \\\n",
       "105780  105780  174363  174364    How can I develop android app?   \n",
       "201841  201841  303951  174364  How can I create an Android app?   \n",
       "363362  363362  493340  493341                               NaN   \n",
       "\n",
       "                                                question2  is_duplicate  \n",
       "105780                                                NaN             0  \n",
       "201841                                                NaN             0  \n",
       "363362  My Chinese name is Haichao Yu. What English na...             0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[train.isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We drop the rows with null values in the questions columns and double check to confirm if it is removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 404287 entries, 0 to 404289\n",
      "Data columns (total 6 columns):\n",
      "id              404287 non-null int64\n",
      "qid1            404287 non-null int64\n",
      "qid2            404287 non-null int64\n",
      "question1       404287 non-null object\n",
      "question2       404287 non-null object\n",
      "is_duplicate    404287 non-null int64\n",
      "dtypes: int64(4), object(2)\n",
      "memory usage: 21.6+ MB\n"
     ]
    }
   ],
   "source": [
    "train = train.dropna(axis=0, how='any')\n",
    "\n",
    "# Verify that rows with null values have been removed\n",
    "train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "17% duplicates are in the test set (as discussed aplenty below):\n",
    "- https://swarbrickjones.wordpress.com/2017/03/28/cross-entropy-and-training-test-class-imbalance/\n",
    "- https://www.kaggle.com/davidthaler/how-many-1-s-are-in-the-public-lb/comments\n",
    "- https://www.kaggle.com/c/quora-question-pairs/discussion/31179\n",
    "\n",
    "Therefore, we try to emulate the same split (17% duplicates, 83% non-duplicates) by rescaling their predictions by the same factor against the training dataset's split (63-37)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## accounting for the discrepancy of the class breakdown of training and testing data\n",
    "def weighted_log_loss(y_true, y_pred):\n",
    "    a = 0.165/0.37\n",
    "    b = (1-0.165)/(1-0.37)\n",
    "    score = a*y_true*np.log(y_pred) + b*(1.0 - y_true)*np.log(1.0 - y_pred)\n",
    "    return -np.mean(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Munging: word2vec embeddings & classical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate labels and data\n",
    "train_X, train_y = train.iloc[:,:-1], train.iloc[:,-1]\n",
    "test_X, test_y = test.iloc[:,:-1], test.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words(qn):\n",
    "    \"\"\"\n",
    "    Returns the number of words in a question\n",
    "    \"\"\"\n",
    "    return len(qn.split())\n",
    "\n",
    "def avg_word_length(qn):\n",
    "    \"\"\"\n",
    "    Tabulates the average word length in a question\n",
    "    \"\"\"\n",
    "    words = qn.split()\n",
    "    return sum([len(word) for word in words])/len(words) if len(words) != 0 else 0\n",
    "\n",
    "def char_count(qn):\n",
    "    \"\"\"\n",
    "    Counts the total number of letters in a question\n",
    "    \"\"\"\n",
    "    return sum([len(word) for word in qn.split()])\n",
    "\n",
    "def caps_count(qn):\n",
    "    \"\"\"\n",
    "    Counts the number of capital letters in a question, \n",
    "    only checking the first word of each sentence\n",
    "    \"\"\"\n",
    "    words = qn.split()\n",
    "    return sum([1 for word in words if word[0].isupper()])\n",
    "\n",
    "# Jaccard Similiarity Coefficient\n",
    "# Obtain the Jaccard Similiarity Coeefficient between 2 questions\n",
    "# (X intersect Y) / (X union Y)\n",
    "# Bag Of Words, which is the list of unique words in the document, with no frequency count.\n",
    "def jaccard_index(row):\n",
    "    \"\"\"\n",
    "    Obtain the Jaccard Similarity Coefficient which essentially is represented\n",
    "    by: (X intersect Y) / (X union Y). Done using the Bag Of Words, \n",
    "    which is the list of unique words in the document, with no frequency count involved. \n",
    "    \n",
    "    Input\n",
    "    ------\n",
    "    row: the row with both questions 1 and 2.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    index: the Jaccard index (AKA Similarity Coefficient)\n",
    "    \"\"\"\n",
    "    q1 = set(row['question1'].split())\n",
    "    q2 = set(row['question2'].split())\n",
    "    index = 1.0\n",
    "    index = (float(len(q1.intersection(q2))) \n",
    "             / len(q1.union(q2)))\n",
    "    return index\n",
    "    \n",
    "\n",
    "def levenshtein(dataframe):\n",
    "    \"\"\"\n",
    "    Obtain the Levensthein distance between the two questions.\n",
    "    Levensthein distance is another similarity index like Jaccard. \n",
    "    \"\"\"\n",
    "    return leven.distance(dataframe['question1'], dataframe['question2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering_classic(df):\n",
    "    qns_set = df.iloc[:,3:5] \n",
    "    q1 = qns_set.iloc[:,0]\n",
    "    q2 = qns_set.iloc[:,1]\n",
    "    \n",
    "    # Creating new features using Feature Engineering\n",
    "    word_len_diff = abs(q1.apply(words) - q2.apply(words))\n",
    "    avg_word_len_diff = abs(q1.apply(avg_word_length) - q2.apply(avg_word_length))\n",
    "    char_diff = abs(q1.apply(char_count) - q2.apply(char_count))\n",
    "    caps_diff = abs(q1.apply(caps_count) - q2.apply(caps_count))\n",
    "    jaccard = qns_set.apply(jaccard_index, axis=1)\n",
    "    leven_dist = qns_set.apply(levenshtein, axis=1)\n",
    "\n",
    "    # Creating a new dataframe with values of new feature\n",
    "    classic_feat = pd.DataFrame({'word_len_diff': word_len_diff, 'avg_word_len_diff': avg_word_len_diff, \n",
    "                                 'char_diff': char_diff, 'caps_diff': caps_diff, 'jaccard': jaccard, \n",
    "                                 'leven_dist': leven_dist})\n",
    "    return classic_feat\n",
    "\n",
    "# classic features for Random Forests classifier\n",
    "classic_train_X = feature_engineering_classic(train_X)\n",
    "classic_test_X = feature_engineering_classic(test_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-Processing word2vec. Derived from Elior Cohen's data cleaning process and building the embedding matrix. Further data cleaning through stopwords to enhance data quality to achieve a better model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text2word(text):\n",
    "    text = str(text).lower()\n",
    "\n",
    "    # Text cleaning\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"e-mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = text.split()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run once ONLY to download stopwords' corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\azhen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare vocabulary of words to be assigned to pre-trained Google corpus' weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\azhen\\Anaconda3\\envs\\ds\\lib\\site-packages\\ipykernel_launcher.py:27: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n"
     ]
    }
   ],
   "source": [
    "# Prepare vocabulary and look up list\n",
    "vocabulary = dict()\n",
    "inverse_vocabulary = ['<unk>']  # placeholder for the [0, 0, ....0] embedding / padding\n",
    "word2vec = KeyedVectors.load_word2vec_format(embedding_file, binary=True)\n",
    "stops = set(stopwords.words(\"english\"))\n",
    "\n",
    "questions_cols = ['question1', 'question2']\n",
    "\n",
    "# Iterate over the questions only of the training dataset\n",
    "for index, row in train_X.iterrows():\n",
    "    # Iterate through the text of both questions of the row\n",
    "    for question in questions_cols:\n",
    "        q2n = []  # q2n -> question to numbers representation\n",
    "        for word in text2word(row[question]):\n",
    "            # Check for unwanted words\n",
    "            if word in stops and word not in word2vec.vocab:\n",
    "                continue\n",
    "            # add new word into vocabulary and assign look up tables with both word & number\n",
    "            if word not in vocabulary:\n",
    "                vocabulary[word] = len(inverse_vocabulary)\n",
    "                q2n.append(len(inverse_vocabulary))\n",
    "                inverse_vocabulary.append(word)\n",
    "            else:\n",
    "                q2n.append(vocabulary[word])\n",
    "\n",
    "        # Replace questions with number representations from vocabulary look-up table\n",
    "        train_X.set_value(index, question, q2n)\n",
    "\n",
    "embedding_dim = 300\n",
    "# This will be the embedding matrix, generated from gaussian distribution for words that do not appear at all. (default values)\n",
    "embeddings = 1 * np.random.randn(len(vocabulary) + 1, embedding_dim)  \n",
    "embeddings[0] = 0  # So that the padding will be ignored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the embedding matrix\n",
    "for word, index in vocabulary.items():\n",
    "    if word in word2vec.vocab:\n",
    "        # adds in the embedding dimension's values per row from Google's pre-trained model\n",
    "        embeddings[index] = word2vec.word_vec(word)\n",
    "\n",
    "del word2vec # to not clog up memory on the system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forests Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\azhen\\Anaconda3\\envs\\ds\\lib\\site-packages\\ipykernel_launcher.py:5: RuntimeWarning: divide by zero encountered in log\n",
      "  \"\"\"\n",
      "C:\\Users\\azhen\\Anaconda3\\envs\\ds\\lib\\site-packages\\ipykernel_launcher.py:5: RuntimeWarning: divide by zero encountered in log\n",
      "  \"\"\"\n",
      "C:\\Users\\azhen\\Anaconda3\\envs\\ds\\lib\\site-packages\\ipykernel_launcher.py:5: RuntimeWarning: divide by zero encountered in log\n",
      "  \"\"\"\n",
      "C:\\Users\\azhen\\Anaconda3\\envs\\ds\\lib\\site-packages\\ipykernel_launcher.py:5: RuntimeWarning: divide by zero encountered in log\n",
      "  \"\"\"\n",
      "C:\\Users\\azhen\\Anaconda3\\envs\\ds\\lib\\site-packages\\ipykernel_launcher.py:5: RuntimeWarning: divide by zero encountered in log\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "# Cross validation with Grid Search to optimize hyper parameters\n",
    "cv_sets = KFold(n_splits=10, random_state=0)\n",
    "scorer = make_scorer(weighted_log_loss, greater_is_better=False)\n",
    "\n",
    "# varying class_weight to penalize False Positives more \n",
    "grid = GridSearchCV(RandomForestClassifier(200, random_state=0),\n",
    "                        scoring=scorer,\n",
    "                        cv = cv_sets,\n",
    "                        param_grid={\"class_weight\": [{0:100, 1:1}, {0:10, 1:1}, {0:1, 1:1}]})\n",
    "\n",
    "# Training Random Forest Classifier with full training dataset\n",
    "grid.fit(classic_train_X, train_y)\n",
    "yhat = grid.predict(classic_test_X)\n",
    "print(\"Random Forests Classifier log loss error: {}\".format(grid.score(classic_test_X, test_y)))\n",
    "print(\"Random Forests Classifier's accuracy: {}\".format(accuracy_score(test_y, yhat)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "pd.crosstab(test_y, yhat, rownames=['Actual Duplicate Qns'], colnames=['Predicted Duplicate Qns'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM-word embedding model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train-Validation data split for x and y values and zero padding of values to ensure consistency in the shape of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# To obtain the max length of the longest question in train/test datasets\n",
    "max_seq_length = max(train_X.question1.map(lambda x: len(x)).max(),\n",
    "                     train_X.question2.map(lambda x: len(x)).max(),\n",
    "                     test_X.question1.map(lambda x: len(x)).max(),\n",
    "                     test_X.question2.map(lambda x: len(x)).max())\n",
    "\n",
    "# Split to dictionaries for training, validation and test data\n",
    "x_train = {'left': x_train.question1, 'right': x_train.question2}\n",
    "x_validation = {'left': x_validation.question1, 'right': x_validation.question2}\n",
    "x_test = {'left': test.question1, 'right': test.question2}\n",
    "\n",
    "# Convert y-values (labels) to their numpy representations\n",
    "y_train = train_y.values\n",
    "y_validation = .values\n",
    "\n",
    "# Zero padding\n",
    "for dataset, side in itertools.product([x_train, x_validation], ['left', 'right']):\n",
    "    dataset[side] = pad_sequences(dataset[side], maxlen=max_seq_length)\n",
    "\n",
    "# Checking consistent shapes for data using assert\n",
    "assert x_train['left'].shape == x_train['right'].shape\n",
    "assert len(x_train['left']) == len(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model building and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_1 (InputLayer)             (None, 213)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_2 (InputLayer)             (None, 213)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)          (None, 213, 300)      36427500    input_1[0][0]                    \n",
      "                                                                   input_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                    (None, 50)            70200       embedding_1[0][0]                \n",
      "                                                                   embedding_1[1][0]                \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)      (None, 100)           0           lstm_1[0][0]                     \n",
      "                                                                   lstm_1[1][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 1)             101         concatenate_1[0][0]              \n",
      "====================================================================================================\n",
      "Total params: 36,497,801\n",
      "Trainable params: 70,301\n",
      "Non-trainable params: 36,427,500\n",
      "____________________________________________________________________________________________________\n",
      "None\n",
      "Train on 323430 samples, validate on 80858 samples\n",
      "Epoch 1/1\n",
      "323430/323430 [==============================] - 3173s - loss: 0.1867 - acc: 0.7202 - val_loss: 0.1763 - val_acc: 0.7409\n"
     ]
    }
   ],
   "source": [
    "# Model hyperparameters\n",
    "hidden_layer_nodes = 50\n",
    "batch_size = 64\n",
    "epochs = 1\n",
    "\n",
    "# declare left and right inputs\n",
    "left_input = Input(shape=(max_seq_length, ), dtype='int32')\n",
    "right_input = Input(shape=(max_seq_length, ), dtype='int32')\n",
    "\n",
    "# Create an embedding layer to convert words to their embeddings\n",
    "embedding_layer = Embedding(len(embeddings), embedding_dim, weights=[embeddings], input_length=max_seq_length, trainable=False)\n",
    "\n",
    "# Convert inputs into word embeddings\n",
    "embedded_left = embedding_layer(left_input)\n",
    "embedded_right = embedding_layer(right_input)\n",
    "\n",
    "# lstm layer that will return an output the size of the number of hidden layer nodes\n",
    "shared_lstm = LSTM(hidden_layer_nodes)\n",
    "\n",
    "# run both inputs through shared lstm\n",
    "encoded_left = shared_lstm(embedded_left)\n",
    "encoded_right = shared_lstm(embedded_right)\n",
    "\n",
    "# concatenate results of both encoded vectors\n",
    "merged_vector = concatenate([encoded_left, encoded_right], axis=-1)\n",
    "\n",
    "# finish off model with output layer\n",
    "prediction = Dense(1, activation='relu')(merged_vector)\n",
    "\n",
    "# Define model hyperparameters such as optimiser and loss function\n",
    "model = Model(inputs=[left_input, right_input], outputs=[prediction])\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['accuracy'])\n",
    "\n",
    "# summarize the model\n",
    "print(model.summary())\n",
    "\n",
    "# Start training\n",
    "model_trained = model.fit([x_train['left'], x_train['right']], y_train, batch_size=batch_size, epochs=epochs,\n",
    "                            validation_data=([x_validation['left'], x_validation['right']], y_validation), verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate model scalar test loss and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80858/80858 [==============================] - 209s   \n",
      "Scalar test loss: 0.176279500335\n",
      "Model accuracy: 0.740928541398\n"
     ]
    }
   ],
   "source": [
    "test_loss_accuracy = model.evaluate([x_validation['left'], x_validation['right']], y_validation)\n",
    "print('Scalar test loss: ' + str(test_loss_accuracy[0]) + '\\nModel accuracy: ' + str(test_loss_accuracy[1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
