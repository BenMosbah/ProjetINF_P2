{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='red'>Quora Question Pairs</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import itertools as itertools\n",
    "import sklearn as skl\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV, KFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, make_scorer, confusion_matrix\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Embedding, LSTM, Merge, Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "import keras.backend as K\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import re\n",
    "import Levenshtein as leven\n",
    "from gensim.models import KeyedVectors\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for reproducability\n",
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import datasets and clean data, for practice.\n",
    "The training dataset provided will be split into train-test to validate model's accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## edit the link to be shorter and accessible from root folder\n",
    "train = pd.read_csv(\"./train.csv\")\n",
    "test = pd.read_csv(\"./test.csv\")\n",
    "embedding_file = \"./GoogleNews-vectors-negative300.bin.gz\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAEICAYAAACj2qi6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nhiSpm6EhSer2/wFmqeHKDYhQ3wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fbb963caa90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicates in the training dataset: 149263\n",
      "Number of non-duplicates in the training dataset: 255027\n",
      "Percentage of duplicates in training dataset: 0.0%\n",
      "Percentage of non-duplicates in training dataset: 100.0%\n"
     ]
    }
   ],
   "source": [
    "count_dups = np.sum(train.iloc[:,-1] == 1)\n",
    "# positive classes' percentage in the training dataset\n",
    "positive_dups = round(count_dups/train.shape[0]*100, 2)\n",
    "\n",
    "%matplotlib inline\n",
    "x = [\"duplicates\", \"non-duplicates\"]\n",
    "plt.bar(x, [count_dups, train.shape[0] - count_dups], align='center')\n",
    "plt.title('Number of Occurences')\n",
    "plt.xticks(x, range(len(x)))\n",
    "plt.yticks()\n",
    "plt.show()\n",
    "\n",
    "print(\"Number of duplicates in the training dataset: {}\".format(count_dups))\n",
    "print(\"Number of non-duplicates in the training dataset: {}\".format(train.shape[0] - count_dups))\n",
    "print(\"Percentage of duplicates in training dataset: {}%\".format(positive_dups))\n",
    "print(\"Percentage of non-duplicates in training dataset: {}%\".format(100.0 - positive_dups))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>404290.000000</td>\n",
       "      <td>404290.000000</td>\n",
       "      <td>404290.000000</td>\n",
       "      <td>404290.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>202144.500000</td>\n",
       "      <td>217243.942418</td>\n",
       "      <td>220955.655337</td>\n",
       "      <td>0.369198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>116708.614502</td>\n",
       "      <td>157751.700002</td>\n",
       "      <td>159903.182629</td>\n",
       "      <td>0.482588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>101072.250000</td>\n",
       "      <td>74437.500000</td>\n",
       "      <td>74727.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>202144.500000</td>\n",
       "      <td>192182.000000</td>\n",
       "      <td>197052.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>303216.750000</td>\n",
       "      <td>346573.500000</td>\n",
       "      <td>354692.500000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>404289.000000</td>\n",
       "      <td>537932.000000</td>\n",
       "      <td>537933.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  id           qid1           qid2   is_duplicate\n",
       "count  404290.000000  404290.000000  404290.000000  404290.000000\n",
       "mean   202144.500000  217243.942418  220955.655337       0.369198\n",
       "std    116708.614502  157751.700002  159903.182629       0.482588\n",
       "min         0.000000       1.000000       2.000000       0.000000\n",
       "25%    101072.250000   74437.500000   74727.000000       0.000000\n",
       "50%    202144.500000  192182.000000  197052.000000       0.000000\n",
       "75%    303216.750000  346573.500000  354692.500000       1.000000\n",
       "max    404289.000000  537932.000000  537933.000000       1.000000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## summary statistics\n",
    "train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 404290 entries, 0 to 404289\n",
      "Data columns (total 6 columns):\n",
      "id              404290 non-null int64\n",
      "qid1            404290 non-null int64\n",
      "qid2            404290 non-null int64\n",
      "question1       404289 non-null object\n",
      "question2       404288 non-null object\n",
      "is_duplicate    404290 non-null int64\n",
      "dtypes: int64(4), object(2)\n",
      "memory usage: 18.5+ MB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2345796 entries, 0 to 2345795\n",
      "Data columns (total 3 columns):\n",
      "test_id      2345796 non-null int64\n",
      "question1    2345794 non-null object\n",
      "question2    2345792 non-null object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 53.7+ MB\n"
     ]
    }
   ],
   "source": [
    "train.info()\n",
    "\n",
    "test.info(null_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>105780</th>\n",
       "      <td>105780</td>\n",
       "      <td>174363</td>\n",
       "      <td>174364</td>\n",
       "      <td>How can I develop android app?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201841</th>\n",
       "      <td>201841</td>\n",
       "      <td>303951</td>\n",
       "      <td>174364</td>\n",
       "      <td>How can I create an Android app?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363362</th>\n",
       "      <td>363362</td>\n",
       "      <td>493340</td>\n",
       "      <td>493341</td>\n",
       "      <td>NaN</td>\n",
       "      <td>My Chinese name is Haichao Yu. What English na...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id    qid1    qid2                         question1  \\\n",
       "105780  105780  174363  174364    How can I develop android app?   \n",
       "201841  201841  303951  174364  How can I create an Android app?   \n",
       "363362  363362  493340  493341                               NaN   \n",
       "\n",
       "                                                question2  is_duplicate  \n",
       "105780                                                NaN             0  \n",
       "201841                                                NaN             0  \n",
       "363362  My Chinese name is Haichao Yu. What English na...             0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[train.isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_id</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>379205</th>\n",
       "      <td>379205</td>\n",
       "      <td>How I can learn android app development?</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>817520</th>\n",
       "      <td>817520</td>\n",
       "      <td>How real can learn android app development?</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>943911</th>\n",
       "      <td>943911</td>\n",
       "      <td>How app development?</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1046690</th>\n",
       "      <td>1046690</td>\n",
       "      <td>NaN</td>\n",
       "      <td>How I what can learn android app development?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1270024</th>\n",
       "      <td>1270024</td>\n",
       "      <td>How I can learn app development?</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1461432</th>\n",
       "      <td>1461432</td>\n",
       "      <td>NaN</td>\n",
       "      <td>How distinct can learn android app development?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         test_id                                    question1  \\\n",
       "379205    379205     How I can learn android app development?   \n",
       "817520    817520  How real can learn android app development?   \n",
       "943911    943911                         How app development?   \n",
       "1046690  1046690                                          NaN   \n",
       "1270024  1270024             How I can learn app development?   \n",
       "1461432  1461432                                          NaN   \n",
       "\n",
       "                                               question2  \n",
       "379205                                               NaN  \n",
       "817520                                               NaN  \n",
       "943911                                               NaN  \n",
       "1046690    How I what can learn android app development?  \n",
       "1270024                                              NaN  \n",
       "1461432  How distinct can learn android app development?  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[test.isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the training dataset, there are 2 null values under feature 'question2' and 1 under feature 'question1'.\n",
    "\n",
    "For the testing dataset, we have 2 null values under feature 'question1' and 4 under feature 'question2'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We drop the rows with null values in the questions columns and double check to confirm if it is removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 404290 entries, 0 to 404289\n",
      "Data columns (total 6 columns):\n",
      "id              404290 non-null int64\n",
      "qid1            404290 non-null int64\n",
      "qid2            404290 non-null int64\n",
      "question1       404290 non-null object\n",
      "question2       404290 non-null object\n",
      "is_duplicate    404290 non-null int64\n",
      "dtypes: int64(4), object(2)\n",
      "memory usage: 18.5+ MB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2345796 entries, 0 to 2345795\n",
      "Data columns (total 3 columns):\n",
      "test_id      2345796 non-null int64\n",
      "question1    2345796 non-null object\n",
      "question2    2345796 non-null object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 53.7+ MB\n"
     ]
    }
   ],
   "source": [
    "train = train.fillna(\"\")\n",
    "test  = test.fillna(\"\")\n",
    "\n",
    "# Verify that rows with null values have been removed\n",
    "train.info()\n",
    "test.info(null_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "17% duplicates are in the test set (as discussed aplenty below):\n",
    "- https://swarbrickjones.wordpress.com/2017/03/28/cross-entropy-and-training-test-class-imbalance/\n",
    "- https://www.kaggle.com/davidthaler/how-many-1-s-are-in-the-public-lb/comments\n",
    "- https://www.kaggle.com/c/quora-question-pairs/discussion/31179\n",
    "\n",
    "Therefore, we try to emulate the same split (17% duplicates, 83% non-duplicates) by rescaling their predictions by the same factor against the training dataset's split (63-37)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## accounting for the discrepancy of the class breakdown of training and testing data\n",
    "def weighted_log_loss(y_true, y_pred):\n",
    "    a = 0.165/0.37\n",
    "    b = (1-0.165)/(1-0.37)\n",
    "    score = a*y_true*np.log(y_pred+0.00001) + b*(1.0 - y_true)*np.log(1.0 - y_pred+0.00001)\n",
    "    return -np.mean(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing and Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# separate labels and data\n",
    "train_X, train_y = train.iloc[:,:-1], train.iloc[:,-1]\n",
    "test_X = test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forests Classifier\n",
    "### Classical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def words(qn):\n",
    "    \"\"\"\n",
    "    Returns the number of words in a question\n",
    "    \"\"\"\n",
    "    return len(qn.split())\n",
    "\n",
    "def avg_word_length(qn):\n",
    "    \"\"\"\n",
    "    Tabulates the average word length in a question\n",
    "    \"\"\"\n",
    "    words = qn.split()\n",
    "    return sum([len(word) for word in words])/len(words) if len(words) != 0 else 0\n",
    "\n",
    "def char_count(qn):\n",
    "    \"\"\"\n",
    "    Counts the total number of letters in a question\n",
    "    \"\"\"\n",
    "    return sum([len(word) for word in qn.split()])\n",
    "\n",
    "def caps_count(qn):\n",
    "    \"\"\"\n",
    "    Counts the number of capital letters in a question, \n",
    "    only checking the first word of each sentence\n",
    "    \"\"\"\n",
    "    words = qn.split()\n",
    "    return sum([1 for word in words if word[0].isupper()])\n",
    "\n",
    "# Jaccard Similiarity Coefficient\n",
    "# Obtain the Jaccard Similiarity Coeefficient between 2 questions\n",
    "# (X intersect Y) / (X union Y)\n",
    "# Bag Of Words, which is the list of unique words in the document, with no frequency count.\n",
    "def jaccard_index(row):\n",
    "    \"\"\"\n",
    "    Obtain the Jaccard Similarity Coefficient which essentially is represented\n",
    "    by: (X intersect Y) / (X union Y). Done using the Bag Of Words, \n",
    "    which is the list of unique words in the document, with no frequency count involved. \n",
    "    \n",
    "    Input\n",
    "    ------\n",
    "    row: the row with both questions 1 and 2.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    index: the Jaccard index (AKA Similarity Coefficient)\n",
    "    \"\"\"\n",
    "    q1 = set(row['question1'].split())\n",
    "    q2 = set(row['question2'].split())\n",
    "    index = 1.0\n",
    "    index = (float(len(q1.intersection(q2))) \n",
    "             / len(q1.union(q2)))\n",
    "    return index\n",
    "    \n",
    "\n",
    "def levenshtein(dataframe):\n",
    "    \"\"\"\n",
    "    Obtain the Levensthein distance between the two questions.\n",
    "    Levensthein distance is another similarity index like Jaccard. \n",
    "    \"\"\"\n",
    "    return leven.distance(dataframe['question1'], dataframe['question2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feature_engineering_classic(df, which):\n",
    "    if which == \"train\":\n",
    "        qns_set = df.iloc[:,3:5] \n",
    "        q1 = qns_set.iloc[:,0]\n",
    "        q2 = qns_set.iloc[:,1]\n",
    "    elif which == \"test\":\n",
    "        qns_set = df.iloc[:,1:]\n",
    "        q1 = df.iloc[:,1]\n",
    "        q2 = df.iloc[:,2]    \n",
    "\n",
    "    # Creating new features using Feature Engineering\n",
    "    word_len_diff = abs(q1.apply(words) - q2.apply(words))\n",
    "    avg_word_len_diff = abs(q1.apply(avg_word_length) - q2.apply(avg_word_length))\n",
    "    char_diff = abs(q1.apply(char_count) - q2.apply(char_count))\n",
    "    caps_diff = abs(q1.apply(caps_count) - q2.apply(caps_count))\n",
    "    jaccard = qns_set.apply(jaccard_index, axis=1)\n",
    "    leven_dist = qns_set.apply(levenshtein, axis=1)\n",
    "\n",
    "    # Creating a new dataframe with values of new feature\n",
    "    classic_feat = pd.DataFrame({'word_len_diff': word_len_diff, 'avg_word_len_diff': avg_word_len_diff, \n",
    "                                 'char_diff': char_diff, 'caps_diff': caps_diff, 'jaccard': jaccard, \n",
    "                                 'leven_dist': leven_dist})\n",
    "    return classic_feat\n",
    "\n",
    "# classic features for Random Forests classifier\n",
    "classic_train_X = feature_engineering_classic(train_X, \"train\")\n",
    "classic_test_X = feature_engineering_classic(test_X, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forests Classifier log loss error: -2.81428114447\n"
     ]
    }
   ],
   "source": [
    "# Cross validation with Grid Search to optimize hyper parameters\n",
    "cv_sets = KFold(n_splits=10, random_state=0)\n",
    "scorer = make_scorer(weighted_log_loss, greater_is_better=False)\n",
    "\n",
    "# varying class_weight to penalize False Positives more \n",
    "grid = GridSearchCV(RandomForestClassifier(200, random_state=0),\n",
    "                        scoring=scorer,\n",
    "                        cv = cv_sets,\n",
    "                        param_grid={\"class_weight\": [{0:100, 1:1}, {0:10, 1:1}, {0:1, 1:1}]})\n",
    "\n",
    "# Training Random Forest Classifier with full training dataset\n",
    "grid.fit(classic_train_X, train_y)\n",
    "print(\"Random Forests Classifier log loss error: {}\".format(grid.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate predictions and submit on kaggle\n",
    "# prob_y = grid.predict_proba(classic_test_X)\n",
    "\n",
    "# submission = pd.DataFrame()\n",
    "# submission['test_id'] = test_X['test_id']\n",
    "# submission['is_duplicate'] = prob_y[:,1]\n",
    "# submission.to_csv(\"submission_rf.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results for benchmark RandomForestClassifier \n",
    "Unfortunately the benchmark scores were not very satisfactory - achieving scores of 0.59123 & 0.58364 on the private and public leaderboard respectively.. Let's attempt the deep learning model (LSTM) and the new and interesting features in the Google corpus' pre-trained embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Siamese LSTM-word embedding model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further Improvements:\n",
    "Pre-Processing word2vec. Derived from Elior Cohen's data cleaning process and building the embedding matrix. Further data cleaning through stopwords to enhance data quality to achieve a better model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def text2word(text):\n",
    "    text = str(text).lower()\n",
    "\n",
    "    # Text cleaning\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"e-mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = text.split()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run once ONLY to download stopwords' corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/zhenxuan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare vocabulary of words to be assigned to pre-trained Google corpus' weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhenxuan/anaconda3/envs/udacity/lib/python2.7/site-packages/ipykernel_launcher.py:28: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n"
     ]
    }
   ],
   "source": [
    "# Prepare vocabulary and look up list\n",
    "vocabulary = dict()\n",
    "inverse_vocabulary = ['<unk>']  # placeholder for the [0, 0, ....0] embedding / padding\n",
    "word2vec = KeyedVectors.load_word2vec_format(embedding_file, binary=True)\n",
    "stops = set(stopwords.words(\"english\"))\n",
    "\n",
    "questions_cols = ['question1', 'question2']\n",
    "\n",
    "# Iterate over the questions only of the training dataset\n",
    "for dataset in [train_X, test_X]:\n",
    "    for index, row in dataset.iterrows():\n",
    "        # Iterate through the text of both questions of the row\n",
    "        for question in questions_cols:\n",
    "            q2n = []  # q2n -> question to numbers representation\n",
    "            for word in text2word(row[question]):\n",
    "                # Check for unwanted words\n",
    "                if word in stops and word not in word2vec.vocab:\n",
    "                    continue\n",
    "                # add new word into vocabulary and assign look up tables with both word & number\n",
    "                if word not in vocabulary:\n",
    "                    vocabulary[word] = len(inverse_vocabulary)\n",
    "                    q2n.append(len(inverse_vocabulary))\n",
    "                    inverse_vocabulary.append(word)\n",
    "                else:\n",
    "                    q2n.append(vocabulary[word])\n",
    "\n",
    "            # Replace questions with number representations from vocabulary look-up table\n",
    "            dataset.set_value(index, question, q2n)\n",
    "\n",
    "embedding_dim = 300\n",
    "# This will be the embedding matrix, generated from gaussian distribution for words that do not appear at all. (default values)\n",
    "embeddings = 1 * np.random.randn(len(vocabulary) + 1, embedding_dim)  \n",
    "embeddings[0] = 0  # So that the padding will be ignored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build the embedding matrix\n",
    "for word, index in vocabulary.items():\n",
    "    if word in word2vec.vocab:\n",
    "        # adds in the embedding dimension's values per row from Google's pre-trained model\n",
    "        embeddings[index] = word2vec.word_vec(word)\n",
    "\n",
    "del word2vec # to not clog up memory on the system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train-Validation data split for x and y values and zero padding of values to ensure consistency in the shape of the data\n",
    "We do a train-validation dataset split so we can have the hold-out test set to be experimented on to improve on our classification model. By including validation data as a \"holdout dataset\" allows me to justify our model's usefulness and effectiveness, due to Kaggle's test set being justified in terms of leaderboard rankings/LB score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# To obtain the max length of the longest question in train/test datasets\n",
    "max_seq_length = max(train_X.question1.map(lambda x: len(x)).max(),\n",
    "                     train_X.question2.map(lambda x: len(x)).max(),\n",
    "                     test_X.question1.map(lambda x: len(x)).max(),\n",
    "                     test_X.question2.map(lambda x: len(x)).max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LSTM_train_X, LSTM_valid_X, LSTM_train_y, LSTM_valid_y = train_test_split(train_X, train_y, test_size=0.2)\n",
    "\n",
    "# Split to dictionaries for training, validation and test data\n",
    "LSTM_train_X = {'left': LSTM_train_X.question1, 'right': LSTM_train_X.question2}\n",
    "LSTM_valid_X = {'left': LSTM_valid_X.question1, 'right': LSTM_valid_X.question2}\n",
    "LSTM_test_X = {'left': test_X.question1, 'right': test_X.question2}\n",
    "\n",
    "# Convert y-values (labels) to their numpy representations\n",
    "LSTM_train_y = LSTM_train_y.values\n",
    "LSTM_valid_y = LSTM_valid_y.values\n",
    "\n",
    "# Zero padding\n",
    "for dataset, side in itertools.product([LSTM_train_X, LSTM_valid_X], ['left', 'right']):\n",
    "    dataset[side] = pad_sequences(dataset[side], maxlen=max_seq_length)\n",
    "\n",
    "# Checking consistent shapes for data using assert\n",
    "assert LSTM_train_X['left'].shape == LSTM_train_X['right'].shape # left and right columns are of the same length\n",
    "assert len(LSTM_train_X['left']) == len(LSTM_train_y) # check if number of rows tally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM model building and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weighted_log_loss(y_true, y_pred):\n",
    "    a = 0.165/0.37\n",
    "    b = (1-0.165)/(1-0.37)\n",
    "    score = a*y_true*K.log(y_pred+0.00001) + b*(1.0 - y_true)*K.log(1.0 - y_pred+0.00001)\n",
    "    return -K.mean(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def exponent_neg_manhattan_distance(left, right):\n",
    "    return K.exp(-K.sum(K.abs(left-right), axis=1, keepdims=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhenxuan/anaconda3/envs/udacity/lib/python2.7/site-packages/ipykernel_launcher.py:25: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 213)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            (None, 213)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 213, 300)     36427800    input_3[0][0]                    \n",
      "                                                                 input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   (None, 50)           70200       embedding_2[0][0]                \n",
      "                                                                 embedding_2[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "merge_2 (Merge)                 (None, 1)            0           lstm_2[0][0]                     \n",
      "                                                                 lstm_2[1][0]                     \n",
      "==================================================================================================\n",
      "Total params: 36,498,000\n",
      "Trainable params: 70,200\n",
      "Non-trainable params: 36,427,800\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Train on 323432 samples, validate on 80858 samples\n",
      "Epoch 1/10\n",
      "323432/323432 [==============================] - 1081s 3ms/step - loss: 0.3500 - acc: 0.6934 - val_loss: 0.3307 - val_acc: 0.7130\n",
      "Epoch 2/10\n",
      "323432/323432 [==============================] - 1084s 3ms/step - loss: 0.3134 - acc: 0.7193 - val_loss: 0.3156 - val_acc: 0.7239\n",
      "Epoch 3/10\n",
      "323432/323432 [==============================] - 1080s 3ms/step - loss: 0.2999 - acc: 0.7324 - val_loss: 0.3079 - val_acc: 0.7329\n",
      "Epoch 4/10\n",
      "323432/323432 [==============================] - 1071s 3ms/step - loss: 0.2907 - acc: 0.7422 - val_loss: 0.3026 - val_acc: 0.7412\n",
      "Epoch 5/10\n",
      "323432/323432 [==============================] - 1077s 3ms/step - loss: 0.2839 - acc: 0.7495 - val_loss: 0.2984 - val_acc: 0.7516\n",
      "Epoch 6/10\n",
      "323432/323432 [==============================] - 1090s 3ms/step - loss: 0.2782 - acc: 0.7555 - val_loss: 0.2962 - val_acc: 0.7468\n",
      "Epoch 7/10\n",
      "323432/323432 [==============================] - 1099s 3ms/step - loss: 0.2738 - acc: 0.7605 - val_loss: 0.2939 - val_acc: 0.7538\n",
      "Epoch 8/10\n",
      "323432/323432 [==============================] - 1095s 3ms/step - loss: 0.2698 - acc: 0.7639 - val_loss: 0.2923 - val_acc: 0.7586\n",
      "Epoch 9/10\n",
      "323432/323432 [==============================] - 1091s 3ms/step - loss: 0.2663 - acc: 0.7678 - val_loss: 0.2922 - val_acc: 0.7690\n",
      "Epoch 10/10\n",
      "323432/323432 [==============================] - 1090s 3ms/step - loss: 0.2631 - acc: 0.7707 - val_loss: 0.2903 - val_acc: 0.7578\n"
     ]
    }
   ],
   "source": [
    "# Model hyperparameters\n",
    "hidden_layer_nodes = 50\n",
    "batch_size = 128\n",
    "epochs = 10\n",
    "\n",
    "# declare left and right inputs\n",
    "left_input = Input(shape=(max_seq_length,), dtype='int32')\n",
    "right_input = Input(shape=(max_seq_length,), dtype='int32')\n",
    "\n",
    "# Create an embedding layer to convert words to their embeddings\n",
    "embedding_layer = Embedding(len(embeddings), embedding_dim, weights=[embeddings], input_length=max_seq_length, trainable=False)\n",
    "\n",
    "# Convert inputs into word embeddings\n",
    "encoded_left = embedding_layer(left_input)\n",
    "encoded_right = embedding_layer(right_input)\n",
    "\n",
    "# lstm layer that will return an output the size of the number of hidden layer nodes\n",
    "shared_lstm = LSTM(hidden_layer_nodes)\n",
    "\n",
    "# run both inputs through shared lstm\n",
    "output_left = shared_lstm(encoded_left)\n",
    "output_right = shared_lstm(encoded_right)\n",
    "\n",
    "# concatenate results of both encoded vectors\n",
    "merged_vector = Merge(mode=lambda x: exponent_neg_manhattan_distance(x[0], x[1]), output_shape=lambda x: (x[0][0], 1))\\\n",
    "                ([output_left, output_right])\n",
    "\n",
    "# finish off model with output layer\n",
    "maLSTM = Model([left_input, right_input], [merged_vector])\n",
    "\n",
    "# optimizer\n",
    "optimizer = Adam(clipnorm=.9)\n",
    "\n",
    "# complete model\n",
    "maLSTM.compile(optimizer=optimizer, loss=weighted_log_loss, metrics=['accuracy'])\n",
    "\n",
    "# summarize the model\n",
    "print(maLSTM.summary())\n",
    "\n",
    "# Start training\n",
    "model_trained = maLSTM.fit([LSTM_train_X['left'], LSTM_train_X['right']], LSTM_train_y, batch_size=batch_size, epochs=epochs,\n",
    "                            validation_data=([LSTM_valid_X['left'], LSTM_valid_X['right']], LSTM_valid_y), verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate model's holdout test loss and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80858/80858 [==============================] - 233s 3ms/step\n",
      "Scalar test loss: 0.290278734785\n",
      "Model accuracy: 0.757822355237\n"
     ]
    }
   ],
   "source": [
    "test_loss_accuracy = maLSTM.evaluate([LSTM_valid_X['left'], LSTM_valid_X['right']], LSTM_valid_y)\n",
    "print('Scalar test loss: ' + str(test_loss_accuracy[0]) + '\\nModel accuracy: ' + str(test_loss_accuracy[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.models import load_model\n",
    "# maLSTM.save('model.h5')\n",
    "# maLSTM = load_model('mode.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM_test_X['left'] = pad_sequences(LSTM_test_X['left'], maxlen=max_seq_length)\n",
    "LSTM_test_X['right'] = pad_sequences(LSTM_test_X['right'], maxlen=max_seq_length)\n",
    "prob_y = maLSTM.predict([LSTM_test_X['left'], LSTM_test_X['right']])\n",
    "\n",
    "## dump into .csv for Kaggle submission\n",
    "submission = pd.DataFrame()\n",
    "submission['test_id'] = test_X['test_id']\n",
    "submission['is_duplicate'] = prob_y\n",
    "submission.to_csv(\"submission_siameseLSTM.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A marked improvement was observed with the subsequent submission on Kaggle, obtaining a public and private score of 0.34697 & 0.35206 respectively. On the private leaderboard that would have meant that I would have been placed in the 1,448th place among 3,308 particpants (top 45% percentile!)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "udacity",
   "language": "python",
   "name": "udacity"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
